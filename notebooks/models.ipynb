{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "import plotly.graph_objs as go\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../instacart-ml'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import common_utility.ModelEvaluation as me\n",
    "import common_utility.PlotlyObject as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def read_spark_csv(spark, path):\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path)\n",
    "    return df\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"My Spark Application\")\\\n",
    "  .config(\"spark.master\", \"local[*]\")\\\n",
    "  .config(\"spark.driver.memory\", \"10g\")\\\n",
    "  .config(\"spark.executor.memory\", \"30g\")\\\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = glob(\"/Users/karenwang/PycharmProjects/instacart-ml/instacart-market-basket-analysis/data/*\")\n",
    "product = read_spark_csv(spark, data_list[0])\n",
    "order = read_spark_csv(spark, data_list[1])\n",
    "order_products_train = read_spark_csv(spark, data_list[2])\n",
    "departments = read_spark_csv(spark, data_list[3])\n",
    "aisles = read_spark_csv(spark, data_list[4])\n",
    "order_products_prior = read_spark_csv(spark, data_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Cohort 1 ####\n",
    "# for every user, collect all previous purchase product_id as # of row in training set\n",
    "order_schema = ['order_id', 'user_id', 'eval_set', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
    "train_user = order.filter(F.col(\"eval_set\") == 'train').select(\"user_id\").distinct()\n",
    "order_product = order_products_prior\\\n",
    "        .unionByName(order_products_train)\\\n",
    "        .join(order.select(*order_schema), ['order_id'], 'inner')\\\n",
    "        .join(train_user, ['user_id'], 'inner')\n",
    "cohort_df = order_product.filter(F.col(\"eval_set\") == \"prior\").groupBy('user_id')\\\n",
    "            .agg(F.collect_set(\"product_id\").alias(\"product_id\"))\\\n",
    "            .withColumn(\"product_id\", F.explode(\"product_id\"))\n",
    "cohort_schema = [\"order_id\", \"product_id\", 'user_id']\n",
    "feature_schema = ['user_id', 'order_id', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
    "\n",
    "train_df = order_product.filter(F.col(\"eval_set\") == 'train').select(*cohort_schema)\\\n",
    "            .join(cohort_df, ['user_id', 'product_id'], 'right')\\\n",
    "            .withColumn(\"reordered\", F.when(F.col(\"order_id\").isNotNull(), 1).otherwise(0)).drop(\"order_id\")\n",
    "output_df = order_product.filter(F.col(\"eval_set\") == 'train') \\\n",
    "            .dropDuplicates([\"user_id\"]).select(*feature_schema) \\\n",
    "            .join(train_df, ['user_id'], 'inner')\\\n",
    "            .join(product.select(\"product_id\", 'department_id'), ['product_id'], 'inner')\n",
    "\n",
    "output_path = \"/Users/karenwang/PycharmProjects/instacart-ml/instacart-market-basket-analysis/parquet/train_df.parquet\"\n",
    "output_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Product level -> Order Level; User_id, Product_id, Reordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(output_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cohort 2 ####\n",
    "df2 = spark.read.parquet(output_path)\n",
    "\n",
    "user_order = order.filter(F.col(\"eval_set\") == 'prior')\\\n",
    "            .groupBy(['user_id']).agg(F.count_distinct(\"order_id\").alias(\"user_order_num\"))\n",
    "\n",
    "# aisle id & reorder rate per product for each user\n",
    "reorder_freq = order_products_prior\\\n",
    "    .join(order.select(\"user_id\", \"order_id\"), [\"order_id\"], \"left\")\\\n",
    "    .groupBy(['user_id', \"product_id\"]).agg(F.count(\"order_id\").alias(\"num_order\"))\\\n",
    "    .join(user_order, [\"user_id\"], \"left\")\\\n",
    "    .withColumn(\"reorder_rate\", F.round(F.col(\"num_order\") / F.col(\"user_order_num\"),2))\\\n",
    "    .join(product.select(\"product_id\", \"aisle_id\"), [\"product_id\"], \"inner\")\n",
    "\n",
    "temp = order.groupBy(\"user_id\").agg(F.mean(\"days_since_prior_order\").alias(\"mean_day\"), \n",
    "                             F.stddev(\"days_since_prior_order\").alias(\"std_day\"))\n",
    "\n",
    "# scale since prior order for each product (based on each user)\n",
    "scale_day_prior = order\\\n",
    "    .filter(F.col(\"eval_set\") == \"train\")\\\n",
    "    .join(temp, [\"user_id\"], \"inner\")\\\n",
    "    .withColumn(\"scale_day_prior\", (F.col(\"days_since_prior_order\") - F.col(\"mean_day\")) / F.col(\"std_day\"))\\\n",
    "    .select(\"user_id\", \"scale_day_prior\")\n",
    "\n",
    "output_df2 = df2.join(reorder_freq.select(\"product_id\", \"user_id\", \"reorder_rate\", \"aisle_id\"), [\"user_id\", \"product_id\"], \"left\")\\\n",
    "    .join(scale_day_prior, ['user_id'], \"left\")\\\n",
    "    .drop(\"days_since_prior_order\").fillna({'scale_day_prior': 0})\n",
    "\n",
    "output_path2 = \"/Users/karenwang/PycharmProjects/instacart-ml/instacart-market-basket-analysis/parquet/train_df2.parquet\"\n",
    "output_df2.write.mode(\"overwrite\").parquet(output_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet(output_path2)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder_list = ['order_dow', 'order_hour_of_day', 'department_id']\n",
    "\n",
    "encoder.fit(df[encoder_list])\n",
    "encoded_data = encoder.transform(df[encoder_list])\n",
    "\n",
    "columns = encoder.get_feature_names_out(encoder_list)\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(encoded_data, columns=columns)\n",
    "df = pd.concat([df, one_hot_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "test_ratio = 0.2\n",
    "n_id = df['user_id'].nunique()\n",
    "test_id = df['user_id'].drop_duplicates().sample(int(n_id *test_ratio)).tolist()\n",
    "train_df = df[~df['user_id'].isin(test_id)].reset_index(drop=True)\n",
    "test_df = df[df['user_id'].isin(test_id)].reset_index(drop=True)\n",
    "positive_rate = train_df[train_df['reordered'] == 0].shape[0] / train_df[\n",
    "    train_df['reordered'] == 1].shape[0] # ratio for imbalance data \n",
    "\n",
    "input_var_list = columns.tolist() + ['days_since_prior_order']\n",
    "label = 'reordered'\n",
    "\n",
    "train_x = train_df[input_var_list]\n",
    "test_x = test_df[input_var_list]\n",
    "train_y = train_df[label]\n",
    "test_y = test_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/Users/karenwang/PycharmProjects/instacart-ml/instacart-market-basket-analysis/parquet/'\n",
    "test_df.to_parquet(path + 'test_prior_training1.parquet', index=False)\n",
    "train_df.to_parquet(path + 'train_prior_training1.parquet', index=False)\n",
    "\n",
    "# Save the test_id list to a file\n",
    "with open(path + 'test_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(test_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=150, n_jobs=-1, verbose=0)\n",
    "logreg.fit(train_x, train_y)\n",
    "\n",
    "test_df['log_prob'] = logreg.predict_proba(test_x)[:,1]\n",
    "test_df['log_pred'] = test_df['log_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "train_df['log_prob'] = logreg.predict_proba(train_x)[:,1]\n",
    "train_df['log_pred'] = train_df['log_prob'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(n_estimators=300, max_depth=6, n_jobs=-1, scale_pos_weight=positive_rate)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# Predict the test set results\n",
    "test_df['xgb_prob'] = clf.predict_proba(test_x)[:,1]\n",
    "test_df['xgb_pred'] = test_df['xgb_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "train_df['xgb_prob'] = clf.predict_proba(train_x)[:,1]\n",
    "train_df['xgb_pred'] = train_df['xgb_prob'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/karenwang/PycharmProjects/instacart-ml/instacart-market-basket-analysis/parquet/'\n",
    "\n",
    "test_df.to_parquet(path + 'test_predictions1.parquet', index=False)\n",
    "train_df.to_parquet(path + 'train_predictions1.parquet', index=False)\n",
    "\n",
    "with open(path + 'loreg1.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg, f)\n",
    "with open(path + 'clf1.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = me.create_roc_trace(train_df, label, 'log_prob', 'train_logistic1')\n",
    "trace2 = me.create_roc_trace(test_df, label, 'log_prob', 'test_logistic1')\n",
    "trace3 = me.create_roc_trace(train_df, label, 'xgb_prob', 'train_xgb1')\n",
    "trace4 = me.create_roc_trace(test_df, label, 'xgb_prob', 'test_xgb1')\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "me.create_overlay_roc_curve(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = me.ClassifierModelEvaluation(train_df, 'logistic_train', label=label, \n",
    "                             pred_col='log_pred', prob_col='log_prob').model_summary(\"logistic_train\")\n",
    "t2 = me.ClassifierModelEvaluation(test_df, 'logistic_test', label=label, \n",
    "                             pred_col='log_pred', prob_col='log_prob').model_summary(\"logistic_test\")\n",
    "t3 = me.ClassifierModelEvaluation(train_df, 'xgb_train', label=label, \n",
    "                             pred_col='log_pred', prob_col='xgb_prob').model_summary(\"xgb_train\")\n",
    "t4 = me.ClassifierModelEvaluation(test_df, 'xgb_test', label=label, \n",
    "                             pred_col='log_pred', prob_col='xgb_prob').model_summary(\"xgb_test\")\n",
    "pd.concat([t1, t2, t3, t4], axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold_list = [i/100 for i in range(40, 65)]\n",
    "b1 = me.create_model_evaluation_by_threshold(train_df, threshold_list, 'logistic_train', label, 'log_prob')\n",
    "b2 = me.create_model_evaluation_by_threshold(test_df, threshold_list, 'logistic_test', label, 'log_prob')\n",
    "b3 = me.create_model_evaluation_by_threshold(train_df, threshold_list, 'xgb_train', label, 'xgb_prob')\n",
    "b4 = me.create_model_evaluation_by_threshold(test_df, threshold_list, 'xgb_test', label, 'xgb_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1.to_parquet(path + 'b1_logistic_train_evaluation.parquet', index=False)\n",
    "b2.to_parquet(path + 'b2_logistic_test_evaluation.parquet', index=False)\n",
    "b3.to_parquet(path + 'b3_xgb_train_evaluation.parquet', index=False)\n",
    "b4.to_parquet(path + 'b4_xgb_test_evaluation.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [plt.create_table_trace(i.round(3).drop('model_name', axis=1)) for i in [b1, b2, b3, b4]]\n",
    "data[0].visible = True\n",
    "\n",
    "var_list = ['logistic_train', 'logistic_test', 'xgb_train', 'xgb_test']\n",
    "buttons = []\n",
    "visible_list = plt.visible_true_false_list(len(var_list), 1)\n",
    "for i in range(len(visible_list)):\n",
    "    temp = {'label': var_list[i],'method': 'update', 'args': [{'visible': visible_list[i]}]}\n",
    "    buttons.append(temp)\n",
    "\n",
    "updatemenus = list([\n",
    "            dict(active=-1,\n",
    "                 x=0.0,\n",
    "                 xanchor='left',\n",
    "                 y=1.33,\n",
    "                 yanchor='top',\n",
    "                 direction='down',\n",
    "                 buttons=buttons,\n",
    "                 )\n",
    "        ])\n",
    "\n",
    "layout = go.Layout(title='<b>Model Performance - Threshold Table<b>',\n",
    "                   updatemenus = updatemenus,\n",
    "                       height=600,\n",
    "                       width=900)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps: \n",
    "- Feature Engineering include adding new predictors. \n",
    "- Used hyperparameter tuning to improve the performance - Tried XGBoost: n_estimator to 500 and 1000, max_depth = 7, 8. But they did not improve the model much. We will try if feature engineering helps improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuring Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider adding the below feature and evaluate the model performance\n",
    "- aisle id\n",
    "- reorder rate for each user per product for each user\n",
    "- sacle since priror order for each product (based on each user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder_list = ['order_dow', 'order_hour_of_day', 'department_id', 'aisle_id']\n",
    "\n",
    "encoder.fit(df2[encoder_list])\n",
    "encoded_data = encoder.transform(df2[encoder_list])\n",
    "\n",
    "columns = encoder.get_feature_names_out(encoder_list)\n",
    "\n",
    "one_hot_encoded_df2 = pd.DataFrame(encoded_data, columns=columns)\n",
    "df2 = pd.concat([df2, one_hot_encoded_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_test_split\n",
    "\n",
    "# Load the test_id list from the file\n",
    "with open(path + 'test_ids.pkl', 'rb') as f:\n",
    "    test_id = pickle.load(f)\n",
    "\n",
    "# Splitting df2 using the loaded test_id\n",
    "train_df = df2[~df2['user_id'].isin(test_id)].reset_index(drop=True)\n",
    "test_df = df2[df2['user_id'].isin(test_id)].reset_index(drop=True)\n",
    "\n",
    "# Calculate the positive rate for the imbalance data\n",
    "positive_rate = train_df[train_df['reordered'] == 0].shape[0] / train_df[train_df['reordered'] == 1].shape[0]\n",
    "\n",
    "# Preparing input and output variables\n",
    "input_var_list = columns.tolist() + ['reorder_rate'] + ['scale_day_prior']\n",
    "label = 'reordered'\n",
    "\n",
    "train_x = train_df[input_var_list]\n",
    "test_x = test_df[input_var_list]\n",
    "train_y = train_df[label]\n",
    "test_y = test_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test_df.to_parquet(path + 'test_prior_training2.parquet', index=False)\n",
    "train_df.to_parquet(path + 'train_prior_training2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=250, n_jobs=-1, verbose=0)\n",
    "logreg.fit(train_x, train_y)\n",
    "\n",
    "test_df['log_prob'] = logreg.predict_proba(test_x)[:,1]\n",
    "test_df['log_pred'] = test_df['log_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "train_df['log_prob'] = logreg.predict_proba(train_x)[:,1]\n",
    "train_df['log_pred'] = train_df['log_prob'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(n_estimators=300, max_depth=6, n_jobs=-1, scale_pos_weight=positive_rate)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# Predict the test set results\n",
    "test_df['xgb_prob'] = clf.predict_proba(test_x)[:,1]\n",
    "test_df['xgb_pred'] = test_df['xgb_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "train_df['xgb_prob'] = clf.predict_proba(train_x)[:,1]\n",
    "train_df['xgb_pred'] = train_df['xgb_prob'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test & train DataFrame with predictions and probabilities\n",
    "test_df.to_parquet(path + 'test_predictions2.parquet', index=False)\n",
    "train_df.to_parquet(path + 'train_predictions2.parquet', index=False)\n",
    "\n",
    "with open(path + 'loreg2.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg, f)\n",
    "with open(path + 'clf2.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace5 = me.create_roc_trace(train_df, label, 'log_prob', 'train_logistic2')\n",
    "trace6 = me.create_roc_trace(test_df, label, 'log_prob', 'test_logistic2')\n",
    "trace7 = me.create_roc_trace(train_df, label, 'xgb_prob', 'train_xgb2')\n",
    "trace8 = me.create_roc_trace(test_df, label, 'xgb_prob', 'test_xgb2')\n",
    "data = [trace5, trace6, trace7, trace8]\n",
    "\n",
    "me.create_overlay_roc_curve(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = me.ClassifierModelEvaluation(train_df, 'logistic_train', label=label, \n",
    "                             pred_col='log_pred', prob_col='log_prob').model_summary(\"logistic_train\")\n",
    "t6 = me.ClassifierModelEvaluation(test_df, 'logistic_test', label=label, \n",
    "                             pred_col='log_pred', prob_col='log_prob').model_summary(\"logistic_test\")\n",
    "t7 = me.ClassifierModelEvaluation(train_df, 'xgb_train', label=label, \n",
    "                             pred_col='log_pred', prob_col='xgb_prob').model_summary(\"xgb_train\")\n",
    "t8 = me.ClassifierModelEvaluation(test_df, 'xgb_test', label=label, \n",
    "                             pred_col='log_pred', prob_col='xgb_prob').model_summary(\"xgb_test\")\n",
    "pd.concat([t5, t6, t7, t8], axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [i/100 for i in range(40, 65)]\n",
    "b5 = me.create_model_evaluation_by_threshold(train_df, threshold_list, 'logistic_train', label, 'log_prob')\n",
    "b6 = me.create_model_evaluation_by_threshold(test_df, threshold_list, 'logistic_test', label, 'log_prob')\n",
    "b7 = me.create_model_evaluation_by_threshold(train_df, threshold_list, 'xgb_train', label, 'xgb_prob')\n",
    "b8 = me.create_model_evaluation_by_threshold(test_df, threshold_list, 'xgb_test', label, 'xgb_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5.to_parquet(path + 'b5_logistic_train_evaluation.parquet', index=False)\n",
    "b6.to_parquet(path + 'b6_logistic_test_evaluation.parquet', index=False)\n",
    "b7.to_parquet(path + 'b7_xgb_train_evaluation.parquet', index=False)\n",
    "b8.to_parquet(path + 'b8_xgb_test_evaluation.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [plt.create_table_trace(i.round(3).drop('model_name', axis=1)) for i in [b5, b6, b7, b8]]\n",
    "data[0].visible = True\n",
    "\n",
    "var_list = ['logistic_train', 'logistic_test', 'xgb_train', 'xgb_test']\n",
    "buttons = []\n",
    "visible_list = plt.visible_true_false_list(len(var_list), 1)\n",
    "for i in range(len(visible_list)):\n",
    "    temp = {'label': var_list[i],'method': 'update', 'args': [{'visible': visible_list[i]}]}\n",
    "    buttons.append(temp)\n",
    "\n",
    "updatemenus = list([\n",
    "            dict(active=-1,\n",
    "                 x=0.0,\n",
    "                 xanchor='left',\n",
    "                 y=1.33,\n",
    "                 yanchor='top',\n",
    "                 direction='down',\n",
    "                 buttons=buttons,\n",
    "                 )\n",
    "        ])\n",
    "\n",
    "layout = go.Layout(title='<b>Model Performance - Threshold Table<b>',\n",
    "                   updatemenus = updatemenus,\n",
    "                       height=600,\n",
    "                       width=900)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "\n",
    "fig = sp.make_subplots(rows=2, cols=2)\n",
    "\n",
    "# Add traces to the subplots\n",
    "fig.add_trace(trace1, row=1, col=1)\n",
    "fig.add_trace(trace5, row=1, col=1)\n",
    "\n",
    "fig.add_trace(trace2, row=1, col=2)\n",
    "fig.add_trace(trace6, row=1, col=2)\n",
    "\n",
    "fig.add_trace(trace3, row=2, col=1)\n",
    "fig.add_trace(trace7, row=2, col=1)\n",
    "\n",
    "fig.add_trace(trace4, row=2, col=2)\n",
    "fig.add_trace(trace8, row=2, col=2)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "a = clf.feature_importances_\n",
    "b = clf.feature_names_in_\n",
    "feature_data = pd.DataFrame({\n",
    "    'Feature': b,\n",
    "    'Importance': a\n",
    "}).round(5)\n",
    "feature_data['nomalized_Feature'] = feature_data['Feature'].str.replace(r'(_\\d+)$', '', regex=True)\n",
    "feature_sum = feature_data.groupby('nomalized_Feature')['Importance'].sum().reset_index(name='feature_sum')\n",
    "\n",
    "# Creating a bar chart using Plotly\n",
    "fig = px.bar(feature_sum, x='nomalized_Feature', y='feature_sum',\n",
    "             title='Normalized Feature Importance',\n",
    "             labels={'nomalized_Feature': 'Feature', 'feature_sum': 'Sum of Importance'})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcgill",
   "language": "python",
   "name": "mcgill"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
